import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

data=pd.read_csv('INFY.csv',parse_dates=['Date'],index_col='Date')
data.head()




# Get the Day and Day of week as features to predict the Closing price of the stock



data['Day']=data.index.day
data['DayOfWeek']=data.index.dayofweek
data.head()


# Plot of the target variable , i.e Close Price


import matplotlib.pyplot as plt
plt.figure(figsize=(30,7))
plt.plot(data.index,data['Close'])
plt.show()


# We take a 90-10 split of the data, You can experiment with the split size.


train_size=int(len(data)*0.9)
train,test=data.iloc[:train_size],data.iloc[train_size:len(data)]
train.shape,test.shape


# Since the features are of various numeric ranges, So with MinMax Scaler we get th variables down to range [0,1].
from sklearn.preprocessing import MinMaxScaler

rs_data = MinMaxScaler()
rs_target = MinMaxScaler()

target=data['Close']
data.drop(columns=['Close'],inplace=True)

train.loc[:,data.columns]=rs_data.fit_transform(train.loc[:,data.columns].to_numpy())
train['Close']=rs_target.fit_transform(train[['Close']].to_numpy())
test.loc[:,data.columns]=rs_data.fit_transform(test.loc[:,data.columns].to_numpy())
test['Close']=rs_target.fit_transform(test[['Close']].to_numpy())


# Here we create the main data., i.e Here we use the previous time stamp data values of the time series as the data and the current time value as target, In the next instance, the current value goes on with the data and the next value is considered as the target.




import numpy as np
def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        v = X.iloc[i:(i + time_steps)].values
        Xs.append(v)
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)


# Here we take the past 5 days close price to predict the 6th day close price with all the other parameters of the data too.




time_steps=5

x_train, y_train = create_dataset(train, train['Close'], time_steps)
x_test, y_test = create_dataset(test, test['Close'], time_steps)

x_train.shape,x_test.shape


# **Model Creation**

# The Conv-1D layer is technically added to smooth out the input vector which is our data. This helps the LSTM underneath to preform really well on the smoothened data.



import keras
model = keras.Sequential()
model.add(keras.layers.Conv1D(filters=32, kernel_size=5,
                      strides=1, padding="causal",
                      activation="relu",
                      input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(
  keras.layers.Bidirectional(
    keras.layers.LSTM(
      units=128,return_sequences=True
    )
  ))
model.add(
  keras.layers.Bidirectional(
  keras.layers.LSTM(
  units=500,return_sequences=True
    )
  ))
model.add(
  keras.layers.Bidirectional(
  keras.layers.LSTM(units=500)
  )
)
model.add(keras.layers.Dropout(rate=0.25))
model.add(keras.layers.Dense(units=100,activation='relu'))
model.add(keras.layers.Dense(10, activation="relu"))
model.add(keras.layers.Dense(units=1))
model.compile(loss=keras.losses.Huber(),
              optimizer='adam',
              metrics=["mse"])


# **Model Training**



history = model.fit(
    x_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.35,
    shuffle=False,
    verbose=1
)


# Since we had scaled the values previously, Now to compare with the actual values and also get correct predictions we have to scale them back to their original range.


pred=model.predict(x_test)
y_train_inv=rs_target.inverse_transform(y_train.reshape(1,-1))
y_test_inv=rs_target.inverse_transform(y_test.reshape(1,-1))
pred=rs_target.inverse_transform(pred.reshape(1,-1))


# **Visualizations**

#  tweaking the hyperparameters such as timesteps and also the train-test-split values.



plt.plot(y_test_inv.flatten(),marker='.',label='True')
plt.plot(pred.flatten(),'r',marker='.',label='Predicted')
plt.legend()
plt.show()




import pickle
pickle_out = open("classifier.pkl","wb")
pickle.dump(model, pickle_out)
pickle_out.close()





model_json = model.to_json()
with open("model.json", "w") as file :
    file.write(model_json)





model.save_weights("weights.h5")




from tensorflow.keras.models import model_from_json

with open("model.json", "r") as file :
    model_json = file.read()

loaded_model = model_from_json(model_json)



loaded_model.load_weights("weights.h5")




loaded_model.predict(x_test)